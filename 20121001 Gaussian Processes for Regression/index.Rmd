---

intelie talk 
============

## Gaussian Processes for regression
### 01/10/2012


---

Machine learning

---

 Supervised learning
 --------------------
 
 the problem of learning input-output mappings from empirical data (the training dataset).
 
 that is:
 
 * we observe some inputs $x_i$ and some outputs $y_i$
 * we assume $y_i = f(x_i)$ for some unknown $f$
 

---

 Regression problem
 --------------

* *Aim*: recover underlying process from **observed data**, allowing prediction of *continuous* quantities.
 
```{r regressionExample, fig.align='center', echo=FALSE, cache=TRUE, fig.height=5} 
x <- -(1:100)/10
y <- 100 + 10 * exp(x / 2) + rnorm(x)/10
nlmod <- nls(y ~  Const + A * exp(B * x), start = list(Const = 1, A = 1, B = 1))
plot(x,y)
curve(100 + 10 * exp(x / 2), col=4, add = TRUE)
lines(x, predict(nlmod), col=2)
```

---

 Probabilistic approach
 ------------------------

* probabilistic models can make predicitions, decisions, etc.
* generative models can be used to handle missing inputs
* uncertantity is a crucial concept

---

 Basic Rules of Probability
 ----------------------------

* probabilities are non-negative $p(x) \ge 0 \forall x$

* probabilities normalize $\sum_{x \in X} p(x) = 1$ 

* join probability of $x$ and $y$ is $p(x,y)$

* mariginal probability of $x$ is: $p(x) = \sum_{y} p(x, y)$

* conditional probability of $x$ given $y$ is $p(x|y) = p(x,y)/p(y)$

* Bayes Rule:
$$
  p(x,y) = p(x) p(y|x) = p(y) p(x|y) \\\
  \implies p(y|x) = \frac{ p(x|y) p(y)  }{ p(x)} 
$$

---

 Expectation and Variance
 ----------------------------

* the *expectation* (mean, average) of a random variable is:

$$ \mu = \mathbb{E}[x] = \int{ x p(x) dx} $$

* the *variance* is:

$$
  \sigma^2 = \mathbb{V}[x] = \int{ (x - \mu)^2 p(x) dx } = \mathbb{E}[x^2] - \mathbb{E}[X]^2
$$


---
 
Univariate Gaussian density $y \in \mathbb{R}, \quad y \sim \mathcal{N}(\mu, \sigma^2)$

```{r normalPlot, fig.align='center', echo=FALSE, cache=TRUE, fig.height=5}
x=seq(-4,4,length=200)
y=dnorm(x,mean=0,sd=1)
plot(x,y,type="l",lwd=2,col="red")
```

$$ p(y|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp { \left( - \frac{ ( y - \mu)^2 }{2\sigma^2} \right) } $$


---

Multivariate Gaussian density $y \in \mathbb{R}^D, \quad y \sim \mathcal{N}(\mathbf{\mu}, \Sigma)$

```{r multivariateNormalPlot, fig.align='center', echo=FALSE, cache=TRUE, fig.height=6}
require("fMultivar")
x = (-40:40)/10
X = grid2d(x)
z = dnorm2d(X$x, X$y, rho = 0)
Z = list(x = x, y = x, z = matrix(z, ncol = length(x)))
persp(Z, theta = -30, phi = 30, col = "steelblue", ticktype="detailed")

```

$$ p(\mathbf{y}|\mathbf{\mu},\Sigma) = \lvert 2\pi\Sigma \rvert^{-\frac{1}{2}} \exp{ \left\[ - \frac{1}{2} ( \mathbf{y} - \mathbf{\mu} )^T \Sigma^{-1} (\mathbf{y} - \mathbf{\mu} )  \right\] } $$




---
  Sampling from a Gaussian distribution
  --------------------------------------
  
* $\mathbf{z} \sim \mathcal{N}(0,1) $

http://en.wikipedia.org/wiki/Normal_distribution#Generating_values_from_normal_distribution


---
  Sampling from a multivariate distribution
  ------------------------------------------

http://www.ideal.ece.utexas.edu/~gjun/ee379k/html/clustering/gmm_em/


--- 
 
 Bayesian learning overview
 ---------------------
 
 * make **prior** assumptions on the value of the parameters **before** we see the data
 
 * **prior** distribution over the parameters: $p(\theta)$
 
 * model of the data given the parameters, **likelihood** function $p(D|\theta)$
 
 * posterior distribution of model parameters:
 
$$ p(\theta | D) = \frac{ p(D|\theta) p(\theta) } {p(D)} $$ 
 
 
 

---


Gaussian Processes
========================


* A Gaussian process is a generalization of a Gaussian distribution to infinitely many random variables 

* Inference takes place directly in function space.

---

A Gaussian process is completely specified by its mean function and co-variance function. 

We define mean function $m(\mathbf{x})$ and the covariance function $k(\mathbf{x}, \mathbf{x}')$ of a real process $f(\mathbf{x})$ as 

$$f(x) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))  $$


--- 

We'll consider the squared exponential (SE) covariance function

$$ cov( f(\mathbf{x}\_p), f(\mathbf{x}\_q) =  k(\mathbf{x}\_p, \mathbf{x}\_q) = \exp( - \frac{1}{2} | \mathbf{x}\_p - \mathbf{x}\_q |^2 ) $$


* which is also known as Radial Basis Function (RBF) kernel.

* covariance between *outputs* is written as function of the *inputs*

* it can be shown that the SE covarience function corresponds to a **Bayesian linear regression model** with an infinite number of basis functions.


--- 

 Bayesian linear regression
 ---------------------------
 
 a GP can be shown to be equivalent to the Bayesian linear regression model.

 Assuming noise $\epsilon \sim \mathcal{N}(0,\sigma^2)$, the linear regression model is: 
 $$ f(\mathbf{x}) = \mathbf{x}^T \mathbf{w}, \quad y = f + \epsilon $$

 and we put a zero mean Gaussian prior with covariance matrix $\Sigma_p$ on the weights:
 
 $$ \mathbf{w} \sim \mathcal{N}(\mathbf{0},\Sigma_p^2) $$
 

---

 Bayesian linear regression 
 -----------------------------------------------

The linear model is extended with function $\phi(\mathbf{x})$ which maps a $D$-dimensional input vector $\mathbf{x}$ into an $N$ dimensional feature space, so now the model becomes:

$$ f(\mathbf{x}) = \phi(\mathbf{x})^T \mathbf{w} $$

this enables the linear model perform well even in non-linear problems.


---

Specification of the covariance function implies a distribution over functions. Given a number of input points $X\_\ast$ we can sample from:

$$ \mathbf{f}\_\ast \sim \mathcal{N}(\mathbf{0}, \mathbf{K}_\ast )  $$


```{r sampleGPs, fig.align='center', echo=FALSE, cache=TRUE, fig.height=4}

require(MASS)
require(plyr)
require(reshape2)
require(ggplot2)

set.seed(12345)

calcSigma <- function(X1,X2,l=1) {
  Sigma <- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1))
  for (i in 1:nrow(Sigma)) {
    for (j in 1:ncol(Sigma)) {
      Sigma[i,j] <- exp(-0.5*(abs(X1[i]-X2[j])/l)^2)
    }
  }
  return(Sigma)
}

x.star <- seq(-5,5,len=50)
sigma <- calcSigma(x.star,x.star)
n.samples <- 3
values <- matrix(rep(0,length(x.star)*n.samples), ncol=n.samples)
for (i in 1:n.samples) {
  values[,i] <- mvrnorm(1, rep(0, length(x.star)), sigma)
}
values <- cbind(x=x.star,as.data.frame(values))
values <- melt(values,id="x")

ggplot(values,aes(x=x,y=value)) +
  geom_rect(xmin=-Inf, xmax=Inf, ymin=-2, ymax=2, fill="grey80") +
  geom_line(aes(group=variable)) +
  theme_bw() +
  scale_y_continuous(lim=c(-2.5,2.5), name="output, f(x)") +
  xlab("input, x")
```

 
 * $\mathbf{K}\_\ast = k(X, X\_\ast)$ 
 
---

 Prediction with noise-free observations
 -----------------------------------------

Consider we know ${(\mathbf{x}\_i, f\_i) | i = 1, \dots, n}$, the joint distribution of the training outputs $\mathbf{f}$ and the test outputs $\mathbf{f}\_\ast$ according to the prior is:

$$ \begin{bmatrix} \mathbf{f} \\\ \mathbf{f}\_\ast \end{bmatrix}
\sim \mathcal{N} \left\( \begin{bmatrix} \mathbf{\mu} \\\ \mathbf{\mu\_\ast} \end{bmatrix},
\begin{bmatrix}
\mathbf{K} &  \mathbf{K}\_\ast \\\
\mathbf{K}\_\ast^T  &  \mathbf{K}\_{\ast\ast} \\\
\end{bmatrix} \right\)
$$

 * $\mathbf{\mu} = ( m(\mathbf{x}\_i), \dots, m(\mathbf{x}\_n) ) $   
 * $K\_{ij} = k(\mathbf{x}\_i,\mathbf{x}\_j)$ is $n \times n$   
 * $\mathbf{K}\_\ast = k(X, X\_\ast)$ 
 * $\mathbf{K}\_{\ast\ast} = k(X\_\ast, X\_\ast)$
  

---

 Prediction with noise-free observations
 -----------------------------------------
 
 By rules for conditioning Gaussians, the posterior has the following form
 
 $$ \begin{aligned} 
 p(\mathbf{f}\_\ast | X\_\ast, X, \mathbf{f}) &= \mathcal{N}( \mathbf{f}\_\ast | \mathbf{\mu}\_\ast, \Sigma\_\ast ) \\\
 \mathbf{\mu}\_\ast &= \mathbf{\mu}(X\_\ast) + \mathbf{K}\_\ast^T \mathbf{K}^{-1} (\mathbf{f} - \mathbf{\mu}(X))  \\\
 \Sigma\_\ast &= \mathbf{K}\_{\ast \ast} - \mathbf{K}\_\ast^T \mathbf{K}^{-1} \mathbf{K}\_\ast  
 \end{aligned} $$ 
 
 
 The mean of the posterior distribution is also its mode, which is also called the *maximum a posteriori* (**MAP**) estimate.
 
---

 GP regression with SE kernel
 ------------------

```{r noiseFreePredictions, fig.align='center', echo=FALSE, cache=TRUE, fig.height=5}

set.seed(12345)
f <- data.frame(x=c(-4,-3,-1,0,2),
                y=c(-2,0,1,2,-1))

# Calculate the covariance matrices
# using the same x.star values as above
x <- f$x
k.xx <- calcSigma(x,x)
k.xxs <- calcSigma(x,x.star)
k.xsx <- calcSigma(x.star,x)
k.xsxs <- calcSigma(x.star,x.star)

# These matrix calculations correspond to equation (2.19)
# in the book.
f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs

# This time we'll plot more samples.  We could of course
# simply plot a +/- 2 standard deviation confidence interval
# as in the book but I want to show the samples explicitly here.
n.samples <- 50
values <- matrix(rep(0,length(x.star)*n.samples), ncol=n.samples)
for (i in 1:n.samples) {
  values[,i] <- mvrnorm(1, f.star.bar, cov.f.star)
}
values <- cbind(x=x.star,as.data.frame(values))
values <- melt(values,id="x")

# Plot the results including the mean function
# and constraining data points
ggplot(values,aes(x=x,y=value)) +
  geom_line(aes(group=variable), colour="grey80") +
  geom_line(data=NULL,aes(x=x.star,y=f.star.bar),colour="red", size=1) + 
  geom_point(data=f,aes(x=x,y=y)) +
  theme_bw() +
  scale_y_continuous(lim=c(-3,3), name="output, f(x)") +
  xlab("input, x")

```

---

 GP regression with SE kernel considering noise
 ----------------------------------------------

```{r noisePredictions, fig.align='center', echo=FALSE, cache=TRUE, fig.height=5}

# The standard deviation of the noise
sigma.n <- 0.1

# Recalculate the mean and covariance functions
f.bar.star <- k.xsx%*%solve(k.xx + sigma.n^2*diag(1, ncol(k.xx)))%*%f$y
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx + sigma.n^2*diag(1, ncol(k.xx)))%*%k.xxs

# Recalulate the sample functions
values <- matrix(rep(0,length(x.star)*n.samples), ncol=n.samples)
for (i in 1:n.samples) {
  values[,i] <- mvrnorm(1, f.bar.star, cov.f.star)
}
values <- cbind(x=x.star,as.data.frame(values))
values <- melt(values,id="x")

# Plot the result, including error bars on the observed points
ggplot(values, aes(x=x,y=value)) + 
  geom_line(aes(group=variable), colour="grey80") +
  geom_line(data=NULL,aes(x=x.star,y=f.bar.star),colour="red", size=1) + 
  geom_errorbar(data=f,aes(x=x,y=NULL,ymin=y-2*sigma.n, ymax=y+2*sigma.n), width=0.2) +
  geom_point(data=f,aes(x=x,y=y)) +
  theme_bw() +
  scale_y_continuous(lim=c(-3,3), name="output, f(x)") +
  xlab("input, x")
```

--- 

 Non parametric method bayesian models
 -----------------------------------

 * very flexibe, state of the art results for regression
 
 * $O(n^3)$ flops, dominated by matrix inverse
 
 * *cholesky* is normally used because it's more numerically stable
 
 * still prohibitive for large $n$
  
 * Iterative methods: *Conjugate Gradient, Jacobi method, Gaussian Belief Propagation*
  
--- 

 Many other covarience functions
 ----------------------------------  
 
 * polynomial
 * MatÃ©rn
 * exponential, $\gamma$-exponential
 * rational quadratic
 * neural network
 
 
--- 
 
 Estimating free (hyper, kernel) parameters
 ----------------------------------
 
 
--- 
 
 Connections to GPs
 ---------------
 
 * SVM
 * Neural Nets
 * Splines
 * and others


---
  References
  ----------

  * C. E. Rasmussen & C. K. I. Williams, Gaussian Processes for Machine Learning,
  * Kevin Murphy, Machine Learning A Probalistic Perspective. 
  * http://www.jameskeirstead.ca/r/gaussian-process-regression-with-r/

